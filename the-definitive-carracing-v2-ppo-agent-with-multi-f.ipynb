{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!apt-get update -qq\n!apt-get install -y swig\n!pip install Box2D-kengz --quiet\n!pip uninstall -y box2d-py\n!pip install gymnasium[box2d] --quiet\n!pip install imageio --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:14:37.104364Z","iopub.execute_input":"2025-02-28T18:14:37.104721Z","iopub.status.idle":"2025-02-28T18:14:53.072209Z","shell.execute_reply.started":"2025-02-28T18:14:37.104696Z","shell.execute_reply":"2025-02-28T18:14:53.071220Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nswig is already the newest version (4.0.2-1ubuntu1).\n0 upgraded, 0 newly installed, 0 to remove and 141 not upgraded.\nFound existing installation: box2d-py 2.3.5\nUninstalling box2d-py-2.3.5:\n  Successfully uninstalled box2d-py-2.3.5\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport imageio\nfrom IPython.display import HTML\nfrom collections import deque\nfrom torch.distributions import Normal\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:02.185985Z","iopub.execute_input":"2025-02-28T18:15:02.186373Z","iopub.status.idle":"2025-02-28T18:15:02.192994Z","shell.execute_reply.started":"2025-02-28T18:15:02.186341Z","shell.execute_reply":"2025-02-28T18:15:02.192109Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"class FrameStackWrapper(gym.Wrapper):\n    def __init__(self, env, num_stack=4):\n        super().__init__(env)\n        self.num_stack = num_stack\n        self.frames = deque([], maxlen=num_stack)\n        orig_shape = self.env.observation_space.shape  # e.g. (96, 96, 3)\n        self.H, self.W, self.C = orig_shape\n        # new shape => (H, W, C * num_stack)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255,\n            shape=(self.H, self.W, self.C * self.num_stack),\n            dtype=env.observation_space.dtype\n        )\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        self.frames.clear()\n        for _ in range(self.num_stack):\n            self.frames.append(obs)\n        return self._get_obs(), info\n    def step(self, action):\n        obs, reward, done, truncated, info = self.env.step(action)\n        self.frames.append(obs)\n        return self._get_obs(), reward, done, truncated, info\n    def _get_obs(self):\n        return np.concatenate(list(self.frames), axis=-1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:10.722317Z","iopub.execute_input":"2025-02-28T18:15:10.722654Z","iopub.status.idle":"2025-02-28T18:15:10.729309Z","shell.execute_reply.started":"2025-02-28T18:15:10.722629Z","shell.execute_reply":"2025-02-28T18:15:10.728476Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"num_stack = 4\nenv_id = \"CarRacing-v2\"\nenv = gym.make(env_id, render_mode=\"rgb_array\")\nenv = FrameStackWrapper(env, num_stack=num_stack)\n\ntest_obs, _ = env.reset()\nprint(\"Test Obs Shape:\", test_obs.shape)  # e.g. (96, 96, 12)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:18.114678Z","iopub.execute_input":"2025-02-28T18:15:18.114995Z","iopub.status.idle":"2025-02-28T18:15:18.156750Z","shell.execute_reply.started":"2025-02-28T18:15:18.114969Z","shell.execute_reply":"2025-02-28T18:15:18.155999Z"}},"outputs":[{"name":"stdout","text":"Test Obs Shape: (96, 96, 12)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, in_channels=12):\n        super(Critic, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64 * 8 * 8, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1)\n        )\n    def forward(self, x):\n        # x shape => (N, 12, 96, 96)\n        x = x.contiguous()  # fix for possible non-contiguous memory\n        x = x / 255.0\n        x = self.conv(x)\n        x = x.contiguous().view(x.size(0), -1)\n        return self.fc(x)\n\nclass Actor(nn.Module):\n    def __init__(self, in_channels=12, action_dim=3):\n        super(Actor, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU()\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(64 * 8 * 8, 512),\n            nn.ReLU()\n        )\n        self.mean_head = nn.Linear(512, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    def forward(self, x):\n        x = x.contiguous()  # ensure contiguous memory\n        x = x / 255.0\n        x = self.conv(x)\n        x = x.contiguous().view(x.size(0), -1)\n        x = self.fc(x)\n        mean = self.mean_head(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:25.012965Z","iopub.execute_input":"2025-02-28T18:15:25.013318Z","iopub.status.idle":"2025-02-28T18:15:25.021542Z","shell.execute_reply.started":"2025-02-28T18:15:25.013291Z","shell.execute_reply":"2025-02-28T18:15:25.020610Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class PPOAgent:\n    def __init__(\n        self,\n        in_channels=12,\n        action_dim=3,\n        lr=1e-4,\n        gamma=0.99,\n        lam=0.95,\n        clip_epsilon=0.3,\n        update_epochs=5,\n        batch_size=32\n    ):\n        self.gamma = gamma\n        self.lam = lam\n        self.clip_epsilon = clip_epsilon\n        self.update_epochs = update_epochs\n        self.batch_size = batch_size\n\n        self.actor = Actor(in_channels=in_channels, action_dim=action_dim).to(device)\n        self.critic = Critic(in_channels=in_channels).to(device)\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n\n    def select_action(self, obs):\n        # obs shape => (96, 96, 12)\n        # We'll permute => (N, C, H, W)\n        obs_tensor = torch.FloatTensor(obs).permute(2, 0, 1).unsqueeze(0).to(device)\n        mean, std = self.actor(obs_tensor)\n        dist = Normal(mean, std)\n        sample = dist.sample()\n        action = torch.clamp(sample, -1.0, 1.0)\n        log_prob = dist.log_prob(sample).sum(dim=-1)\n        value = self.critic(obs_tensor)\n        return (\n            action.detach().cpu().numpy().flatten(),\n            log_prob.detach().cpu().numpy().flatten(),\n            value.detach().cpu().numpy().flatten()\n        )\n\n    def compute_gae(self, rewards, dones, values):\n        advantages = []\n        gae = 0\n        next_value = 0\n        for i in reversed(range(len(rewards))):\n            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]\n            gae = delta + self.gamma * self.lam * gae * (1 - dones[i])\n            next_value = values[i]\n            advantages.insert(0, gae)\n        return advantages\n\n    def evaluate_actions(self, obs_batch, act_batch):\n        # obs_batch => (N, 96, 96, 12)\n        # permute => (N, 12, 96, 96)\n        obs_tensor = torch.FloatTensor(obs_batch).permute(0, 3, 1, 2).to(device)\n        act_tensor = torch.FloatTensor(act_batch).to(device)\n\n        mean, std = self.actor(obs_tensor)\n        dist = Normal(mean, std)\n        log_probs = dist.log_prob(act_tensor).sum(dim=-1)\n        entropy = dist.entropy().sum(dim=-1)\n        values = self.critic(obs_tensor)\n        return log_probs, entropy, values\n\n    def update(self, trajectories):\n        obs_list = np.array([t[0] for t in trajectories])\n        act_list = np.array([t[1] for t in trajectories])\n        rew_list = np.array([t[2] for t in trajectories])\n        old_logp_list = np.array([t[3] for t in trajectories])\n        val_list = np.array([t[4] for t in trajectories])\n        done_list = np.array([t[5] for t in trajectories])\n\n        advantages = self.compute_gae(rew_list, done_list, val_list)\n        advantages = np.array(advantages)\n        returns = advantages + val_list\n\n        # advantage normalization\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        dataset_size = len(trajectories)\n        indices = np.arange(dataset_size)\n\n        for _ in range(self.update_epochs):\n            np.random.shuffle(indices)\n            start_idx = 0\n            while start_idx < dataset_size:\n                end_idx = min(start_idx + self.batch_size, dataset_size)\n                batch_idx = indices[start_idx:end_idx]\n                start_idx = end_idx\n\n                mb_obs = obs_list[batch_idx]\n                mb_act = act_list[batch_idx]\n                mb_old_logp = torch.FloatTensor(old_logp_list[batch_idx]).to(device)\n                mb_returns = torch.FloatTensor(returns[batch_idx]).to(device)\n                mb_adv = torch.FloatTensor(advantages[batch_idx]).to(device)\n\n                log_probs, entropy, values = self.evaluate_actions(mb_obs, mb_act)\n\n                ratio = torch.exp(log_probs - mb_old_logp)\n                surr1 = ratio * mb_adv\n                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_adv\n\n                # Actor\n                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy.mean()\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n\n                # Critic\n                critic_loss = nn.MSELoss()(values.squeeze(1), mb_returns)\n                self.critic_optimizer.zero_grad()\n                critic_loss.backward()\n                self.critic_optimizer.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:32.038573Z","iopub.execute_input":"2025-02-28T18:15:32.038888Z","iopub.status.idle":"2025-02-28T18:15:32.053219Z","shell.execute_reply.started":"2025-02-28T18:15:32.038863Z","shell.execute_reply":"2025-02-28T18:15:32.052332Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def ultra_reward_shaping(obs, original_reward, done, truncated):\n    \"\"\"\n    1) A small living reward (0.1).\n    2) If original_reward > 0 => +0.2 to boost good driving steps.\n    3) Check a center row for 'gray' track color => if ratio > 0.5 => +0.3, else -0.5\n    4) Episode end => -10\n    \"\"\"\n    shaped = original_reward + 0.1\n    if original_reward > 0:\n        shaped += 0.2\n    h, w, c = obs.shape\n    center_row = obs[h//2, :, :]\n    gray_approx = np.mean(center_row, axis=1)\n    gray_pixels = ((gray_approx >= 110) & (gray_approx <= 140)).sum()\n    ratio_gray = gray_pixels / w\n    if ratio_gray > 0.5:\n        shaped += 0.3\n    else:\n        shaped -= 0.5\n    if done or truncated:\n        shaped -= 10.0\n    return shaped\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:46.180178Z","iopub.execute_input":"2025-02-28T18:15:46.180507Z","iopub.status.idle":"2025-02-28T18:15:46.185568Z","shell.execute_reply.started":"2025-02-28T18:15:46.180480Z","shell.execute_reply":"2025-02-28T18:15:46.184777Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"agent = PPOAgent(in_channels=num_stack*3, action_dim=3, lr=1e-4, clip_epsilon=0.3, update_epochs=5, batch_size=32)\nnum_episodes = 300\nmax_timesteps = 1000\nall_rewards = []\n\nfor episode in range(num_episodes):\n    obs, _ = env.reset()\n    ep_reward = 0\n    trajectories = []\n    for t in range(max_timesteps):\n        action, logp, val = agent.select_action(obs)\n        next_obs, reward, done, truncated, _ = env.step(action)\n        shaped_reward = ultra_reward_shaping(obs, reward, done, truncated)\n        trajectories.append((obs, action, shaped_reward, logp, val, float(done or truncated)))\n        obs = next_obs\n        ep_reward += shaped_reward\n        if done or truncated:\n            break\n    all_rewards.append(ep_reward)\n    agent.update(trajectories)\n    print(f\"Episode {episode}, Shaped Reward: {ep_reward}\")\n\nplt.figure(figsize=(10,6))\nplt.plot(all_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Shaped Reward\")\nplt.title(\"CarRacing-v2 PPO with FrameStack & Contiguous Fix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T18:15:53.415113Z","iopub.execute_input":"2025-02-28T18:15:53.415414Z","execution_failed":"2025-02-28T18:22:31.024Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n","output_type":"stream"},{"name":"stdout","text":"Episode 0, Shaped Reward: 218.014035087724\nEpisode 1, Shaped Reward: 302.4680134680195\nEpisode 2, Shaped Reward: 295.9146579804619\nEpisode 3, Shaped Reward: 296.77894736842694\nEpisode 4, Shaped Reward: 296.2493150684991\nEpisode 5, Shaped Reward: 294.2727272727331\nEpisode 6, Shaped Reward: 296.14716981132665\nEpisode 7, Shaped Reward: 293.36942675159816\nEpisode 8, Shaped Reward: 293.849315068499\nEpisode 9, Shaped Reward: 293.43086816720836\nEpisode 10, Shaped Reward: 292.7529411764763\nEpisode 11, Shaped Reward: 294.06713780919307\nEpisode 12, Shaped Reward: 293.4724919093909\nEpisode 13, Shaped Reward: 296.0666666666726\nEpisode 14, Shaped Reward: 296.5684587813679\nEpisode 15, Shaped Reward: 295.40336134454367\nEpisode 16, Shaped Reward: 296.9567567567627\nEpisode 17, Shaped Reward: 296.5174377224258\nEpisode 18, Shaped Reward: 296.8666666666726\nEpisode 19, Shaped Reward: 296.3204152249194\nEpisode 20, Shaped Reward: 296.0006600660125\nEpisode 21, Shaped Reward: 297.4992700729986\nEpisode 22, Shaped Reward: 294.940166204992\nEpisode 23, Shaped Reward: 297.21754385965505\nEpisode 24, Shaped Reward: 296.5174377224258\nEpisode 25, Shaped Reward: 293.6666666666725\nEpisode 26, Shaped Reward: 296.0006600660125\nEpisode 27, Shaped Reward: 294.0422535211326\nEpisode 28, Shaped Reward: 293.63090379009327\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"env_vis = gym.make(\"CarRacing-v2\", render_mode=\"rgb_array\")\nenv_vis = FrameStackWrapper(env_vis, num_stack=num_stack)\ns, _ = env_vis.reset()\nframes = []\nfor t in range(1200):\n    frame = env_vis.render()\n    frames.append(frame)\n    a, _, _ = agent.select_action(s)\n    s_next, r, d, trunc, _ = env_vis.step(a)\n    s = s_next\n    if d or trunc:\n        break\nenv_vis.close()\nimageio.mimsave('carracing_ppo.gif', frames, fps=30)\nHTML('<img src=\"carracing_ppo.gif\">')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}